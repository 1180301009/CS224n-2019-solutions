<a href="https://996.icu"><img src="https://img.shields.io/badge/link-996.icu-red.svg" alt="996.icu"></a>

# CS224n-winter19

Solutions for **CS224n, winter, 2019.**    
Welcome to discuss problems appearing in assigments, **please** submit to issue.    
Also take notes for the key point in lectures.
The solutions for assignment is written by **Markdown**.
&nbsp;
- Course page: https://web.stanford.edu/class/cs224n
- Video page: https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z


<!-- CS224n-win-2019练习答案。    
问题请提交至issue，欢迎各位一起讨论。    
需要书写的答案都是用**Markdown**写的，内容为全英文。    
水平有限，望各位不吝指教。    
部分答案有缺失，紧急补课中···     -->
***

## w1
- [x] Assignment1
- [x] a1
- [x] Gensim
- [x] note:&ensp;Word Vectors I: Introduction, SVD and Word2Ve
- [x] reading:&ensp;Word2Vec Tutorial - The Skip-Gram Model
&nbsp;

## w2

- [x] Assignment2
- [x] a2
- [x] note:&ensp;Word Vectors II: GloVe, Evaluation and Trainin
- [x] reading:&ensp;gradient-notes
- [x] reading:&ensp;CS231n notes on backprop
- [ ] reading:&ensp;review-differential-calculus
- [x] reading:&ensp;backprop_old
- [ ] reading:&ensp;CS231n notes on network architectures
&nbsp;

## **w3**

- [x] note:&ensp;Dependency Parsing 
- [x] note:&ensp;Language Models and Recurrent Neural Network
- [x] a3
- [x] Assignment3

## **w4**

- [x] note:&emsp;Machine Translation, Sequence-to-sequence and Attention
- [x] a4
- [ ] Assignment4
- [x] read:&emsp;Attention and Augmented Recurrent Neural Networks

**key point for a4:**
How to understand pack_padded_sequence and pad_packed_sequence?    
[(Chinese ed)](https://blog.csdn.net/lssc4205/article/details/79474735)    
[(English ed)](https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec)

